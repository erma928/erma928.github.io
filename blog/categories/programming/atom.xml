<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | 季米风]]></title>
  <link href="http://www.fengjimin.com/blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://www.fengjimin.com/"/>
  <updated>2019-01-22T20:13:29+08:00</updated>
  <id>http://www.fengjimin.com/</id>
  <author>
    <name><![CDATA[冯继敏]]></name>
    <email><![CDATA[joe5338.cn@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[python machine learning and text processing (2)]]></title>
    <link href="http://www.fengjimin.com/blog/2019/01/22/python-machine-learning-and-text-processing-2/"/>
    <updated>2019-01-22T16:44:00+08:00</updated>
    <id>http://www.fengjimin.com/blog/2019/01/22/python-machine-learning-and-text-processing-2</id>
    <content type="html"><![CDATA[<p>机器学习是个大课题，也是一种解决现实问题的思维模式。虽然单纯学习其算法不算太难，然而要有对解决实际问题的纯熟运用，却需要在理论与实践相结合方面下足功夫。python在这方面提供了足够便利的工具。</p>

<!--more-->


<h3>简介</h3>

<ol>
<li>监督学习

<ol>
<li>分类问题</li>
<li>回归问题</li>
</ol>
</li>
<li>非监督学习

<ol>
<li>聚类</li>
<li>降维</li>
</ol>
</li>
</ol>


<h3>步骤</h3>

<ol>
<li>收集数据

<ol>
<li><a href="https://data.world/crowdflower/disasters-on-social-media">Disasters on Social Media</a> 数据集</li>
</ol>
</li>
<li>数据清洗

<ol>
<li>去除不相关字符</li>
<li>拆分独立单词</li>
<li>去除不相关词语</li>
<li>转换成小写</li>
<li>多种拼法御特定表达绑定</li>
<li>词型还原</li>
</ol>
</li>
<li>特征提取

<ol>
<li>bag of words 词袋法

<ol>
<li><code>John is happy he is not hungary for apples. -&gt; [0,2,1,1,1,1,1,1,1]</code></li>
<li>完全忽略单词顺序</li>
</ol>
</li>
<li>one-hot

<ol>
<li><code>John is happy he is not hungary for apples. -&gt; [0,1,1,1,1,1,1,1,1]</code></li>
</ol>
</li>
<li>TF-IDF

<ol>
<li><img src="http://www.ruanyifeng.com/blogimg/asset/201303/bg2013031504.png" alt="TF" /></li>
<li><img src="http://www.ruanyifeng.com/blogimg/asset/201303/bg2013031506.png" alt="IDF" /></li>
</ol>
</li>
<li>word2vec

<ol>
<li>使用浅层双层神经网络，映射每一个词到一个向量，使之能表达词语之间的相互关系，以及上下文的关系。如：<code>V(queen) - V(woman) + V(man) = V(king)</code></li>
<li>训练算法

<ol>
<li>skip-grams: 由当前单词预测上下文</li>
<li>CBOW: 由上下文预测当前单词</li>
<li><img src="https://static.leiphone.com/uploads/new/article/740_740/201706/594b31d0920ef.png?imageMogr2/format/jpg/quality/90" alt="nn" /></li>
<li><img src="https://static.leiphone.com/uploads/new/article/740_740/201706/594b3267c64f4.png?imageMogr2/format/jpg/quality/90" alt="weights" /> 中间300weights就是car单词的词向量</li>
<li>句子嵌入的向量表示，可以采用将句子中词汇的word2vec平均化</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>分类

<ol>
<li>数据集划分为训练集和测试集</li>
<li>分类算法：逻辑回归，SVM，贝叶斯公式，等等</li>
</ol>
</li>
<li>检验效果

<ol>
<li>混淆矩阵</li>
<li>precision, recall, f1_score..</li>
</ol>
</li>
</ol>


<h3>Scikit-learn库</h3>

<ol>
<li><p>算法包</p>

<table>
<thead>
<tr>
<th></th>
<th> 导入算法     </th>
<th> 代码                                               </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> 广义线性模型 </td>
<td> from sklearn.linear_model import LinearRegression  |</td>
</tr>
<tr>
<td></td>
<td> 逻辑回归     </td>
<td> rom sklearn.linear_model import LogisticRegression |</td>
</tr>
<tr>
<td></td>
<td> 支持向量机   </td>
<td> from sklearn.svm import SVC                        |</td>
</tr>
<tr>
<td></td>
<td> k近邻        </td>
<td> from sklearn.neighbors import KNeighborsClassifier |</td>
</tr>
<tr>
<td></td>
<td> 决策树       </td>
<td> from sklearn.tree import DecisionTreeClassifier    |</td>
</tr>
<tr>
<td></td>
<td> 朴素贝叶斯   </td>
<td> from sklearn.naive_bayes import MultinomialNB      |</td>
</tr>
</tbody>
</table>
</li>
<li><p>转换器（transformer）：用于数据预处理和数据转换</p>

<ol>
<li>transformer.fit</li>
<li>transformer.transform</li>
<li>transformer.fit_transform</li>
</ol>
</li>
<li><p>估计器（estimator）：用于分类和聚类</p>

<ol>
<li>estimator.fit</li>
<li>estimator.predict</li>
</ol>
</li>
<li><p>特征提取（sklearn.feature_extraction）</p>

<ol>
<li>sklearn.feature_extraction.text.CountVectorizer：将文本转换为每个词出现的个数的向量（词典法）</li>
<li>sklearn.feature_extraction.text.TfidfVectorizer：将文本转换为tfidf值的向量</li>
</ol>
</li>
<li><p>数据集分割（sklearn.cross_validation）</p>

<ol>
<li>train_test_split(X, y, test_size, random_state)</li>
</ol>
</li>
<li><p>效果评估（sklearn.metrics）</p>

<ol>
<li>confusion_matrix</li>
<li>accuracy_score</li>
<li>classification_report</li>
<li>precision_recall_fscore_support</li>
</ol>
</li>
</ol>


<h3>sklearn文本特征提取</h3>

<p>```python
from sklearn.feature_extraction.text import CountVectorizer</p>

<p>corpus = [&ldquo;Hey lets get lunch :)&rdquo;,</p>

<pre><code>       "Hey!!! I need a favor"]
</code></pre>

<p>vectorize = CountVectorizer()
vectorize.fit(corpus)</p>

<h1>构建 文档词频矩阵</h1>

<p>dtm = vectorize.transform(corpus)
```</p>

<p>```python
from sklearn.feature_extraction.text import TfidfVectorizer</p>

<p>def createDTM(corpus):</p>

<pre><code>"""构建文档词语矩阵"""
vectorize = TfidfVectorizer()
#注意fit_transform相当于fit之后又transform。
dtm = vectorize.fit_transform(corpus)
return dtm
</code></pre>

<p>corpus = [&ldquo;Hey lets get lunch :)&rdquo;,</p>

<pre><code>       "Hey!!! I need a favor"]
</code></pre>

<p>createDTM(corpus)
```</p>

<h3>sklearn文本分类</h3>

<p>```python
import re
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split</p>

<p>def normalize_numbers(s):</p>

<pre><code>"""
将数字特征统一替换为NUM
"""
return re.sub(r'\b\d+\b', 'NUM', s)
</code></pre>

<p>dataset = pd.read_csv(&lsquo;data/20news-18828.csv&rsquo;, header=None, delimiter=&lsquo;,&rsquo;, names=[&lsquo;label&rsquo;, &lsquo;text&rsquo;])</p>

<h1>依次导入X， y， test_size</h1>

<p>X_train, X_test, y_train, y_test = train_test_split(dataset.text, dataset.label, test_size=0.3)</p>

<h1>CountVectorizer和TfidfVectorizer都有preprocessor参数，该参数传入预处理函数。</h1>

<h1>也含有停用词处理参数stop_words</h1>

<p>tv = TfidfVectorizer(preprocessor=normalize_numbers, stop_words=&lsquo;english&rsquo;)
X_train_tf = tv.fit_transform(X_train)
X_test_tf = tv.transform(X_test)</p>

<h1>贝叶斯分类器</h1>

<p>estimator2 = MultinomialNB()
estimator2.fit(X_train_tf, y_train)
predicted = estimator2.predict(X_test_tf)</p>

<p>print(classification_report(y_test, predicted, labels=dataset.label.unique()))</p>

<h1>逻辑回归分类器</h1>

<p>LR = LogisticRegression()
LR.fit(X_train_tf, y_train)
predicted = LR.predict(X_test_tf)
print(classification_report(y_test, predicted, labels=dataset.label.unique()))
```</p>

<h3>sklearn话题分析（聚类）</h3>

<p>```python
import csv
import jieba
import re
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import pandas as pd</p>

<p>def read_data(file):</p>

<pre><code>"""
读取csv数据，返回含有字符串的列表数据。
:param file:  csv文件
:return: 列表数据（列表中每个组成部分是字符串文本）
"""

df = pd.read_excel(file)
#从dataframe中读取  news_content  列的所有新闻信息
documents = df['news_content']
return documents
</code></pre>

<p>def cluster_analysis(best_k, texts):</p>

<pre><code>"""
按照最佳聚类数k，将texts聚成k类。
:param best_k: 聚类准确率最好的 k值
:param texts: 列表数据（且列表中每个组成部分是字符串文本）
:return:None

"""

documents = [' '.join(jieba.lcut(text)) for text in texts]

tfidf = TfidfVectorizer(max_df=0.5)
# 学习tfidf矩阵数据中的分布规律
tfidf.fit(documents)
matrix = tfidf.transform(documents)
print('文档向量化矩阵：')
print(matrix.toarray())
# c初始化Kmeans估计器
estimator = KMeans(n_clusters=best_k)
estimator.fit(matrix)

#机器给所有的文档打上的标签
#此处self.label_pred 为一维列表，列表长度等于 文档的数目
label_pred = estimator.labels_

#保存聚类算法的分类结果 到output文件夹中。
with open('data/cluster_output.csv', 'a+', encoding='gbk', newline='') as csvf:
    writer = csv.writer(csvf)
    writer.writerow(('news_content', 'cluster'))
    #将新闻与 无kmeans聚类结果（cluster）写入csv文件
    for idx, text in enumerate(texts):
        try:
            writer.writerow((text, label_pred[idx]))
        except:
            continue
</code></pre>

<p>texts = read_data(file=&lsquo;data/百度新闻.xlsx&rsquo;)
cluster_analysis(best_k=19, texts=texts)              <br/>
```</p>

<h3>SVD计算消费者偏好</h3>

<ol>
<li>推荐算法比较常见的实现方式是协同过滤（Collaberative Filtering）。数据涉及到用户，产品，和产品评价三种信息。</li>
<li>有了用户评价矩阵，可以采用奇异值分解（SVD），获得降维后的用户向量，从而更准确的计算得到推荐信息</li>
<li>pip install scikit-surprise</li>
</ol>


<p>```python
from surprise import Reader, Dataset
from surprise import SVD, evaluate</p>

<h1>定义数据格式</h1>

<p>reader = Reader(line_format=&lsquo;user item rating timestamp&rsquo;, sep=&lsquo;\t&rsquo;)</p>

<h1>使用reader格式从u.data文件中读取数据</h1>

<p>data = Dataset.load_from_file(&lsquo;data/u.data&rsquo;, reader=reader)
data.split(n_folds=5)</p>

<h1>相当于scikit的机器学习算法的初始化</h1>

<p>svd = SVD()</p>

<h1>相当于scikit中的score，模型评估</h1>

<p>evaluate(svd, data, measures=[&lsquo;RMSE&rsquo;, &lsquo;MAE&rsquo;])</p>

<p>data = data.build_full_trainset()
svd.fit(data)
svd.predict(186, 302, 4)
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[python machine learning and text processing (1)]]></title>
    <link href="http://www.fengjimin.com/blog/2019/01/22/python-machine-learning-and-text-processing-1/"/>
    <updated>2019-01-22T15:32:00+08:00</updated>
    <id>http://www.fengjimin.com/blog/2019/01/22/python-machine-learning-and-text-processing-1</id>
    <content type="html"><![CDATA[<h3>前言</h3>

<p>近期在学习python和机器学习。其实本人大学时学过不少机器学习相关专业课，包括神经网络，计算视觉与图像处理，模式识别，自然语言处理（NLP）等。也做过相关研究，读了些paper。可以说有一定基础。只是工作之后没有特意往这个方向发展，不自觉走入了被各种业务需求埋没的境地。如今AI大热，自己也对这块兴趣盎然，因而特别想在理论和实践上弄通。</p>

<!--more-->


<p>若干年以前，因Ruby on Rails快速开发的特性，自己曾把ruby当作银弹，很痴迷于这门语言。也看了很多python与ruby谁更强大这方面的争论。当时觉得ruby的表达能力很强大且优美，近乎魔术，而python，光使用缩进语法这一点就觉得有点怪，因而并没有深入去学。虽然其解决问题有且只有一条最佳思路、还有make things explicit的理念听起来有些道理。然而世事难料，随着Rails渐趋冷静，ruby似乎也渐趋没落。而python，随着人工智能的兴起却在走向潮头。当然，自己现在来学python，已经没有了当初的门户之见，反倒很容易的见识到python简单易用，以及确实强大的方面。在大数据和人工智能的浪潮中，python能站在前沿，是有它的独到之处的。</p>

<p>本blog主要对python文本处理基础，以及机器学习相关的内容进行提纲挈领。其中文本处理包含了分词、自然语言统计、可视化、数据分析等python库。而机器学习，则包含其理论分野、实现步骤、基本机器学习算法和python库等。</p>

<p>以下为文本处理基础库相关内容。</p>

<h3>分词</h3>

<ol>
<li><p>jieba 中文分词库</p>

<ol>
<li>cut, lcut</li>
<li>load_userdict(file)</li>
<li>jieba.posseg.cut</li>
</ol>
</li>
<li><p>正则词频统计</p>

<ol>
<li><code>words = re.findall(r'[\u4e00-\u9fa5]+', test)</code> 匹配中文正则</li>
</ol>
</li>
</ol>


<p>```python
for word in wordset:</p>

<pre><code>if len(word)&gt;1 :
    freq = wordlist.count(word)
    result.append((word, freq))         
</code></pre>

<h1>[(w1,freq1),(w2,freq2)&hellip;.]</h1>

<h1>对词频统计结果进行排序</h1>

<p>new_result = sorted(result, key=lambda k:k[1], reverse=True)
```</p>

<h3>自然语言统计</h3>

<ol>
<li><p>nltk 自然语言处理库</p>

<ol>
<li><p>nltk.text.Text类</p>

<ol>
<li><code>text = Text(wordlist)</code></li>
<li><code>text.concordance(word='汪淼', width=20, lines=10)</code> 上下文</li>
<li><code>text.similar(word='叶文洁', num=10)</code>上下文最相关词</li>
<li><code>text.count(word='叶文洁')</code> 统计词频</li>
<li><code>text.dispersion_plot(words)</code> 离散图，横坐标表示词语的索引位置</li>
</ol>
</li>
<li><p>Ngrams的实现</p>

<ol>
<li><code>nltk.util.bigrams(seq), trigrams(seq), ngrams(seq, n), skipgrams(seq, n, k)</code></li>
</ol>
</li>
<li><p>nltk.probability</p>

<ol>
<li><p>FreqDist</p>

<ol>
<li><code>fdist = FreqDist(wordlist)</code></li>
<li><code>fdist.items()</code> 词频对</li>
<li><code>fdist.N()</code> 词语总数</li>
<li><code>fdist.max()</code> 词语总数</li>
<li><code>fdist.freq('三体')</code> 词频占比</li>
<li><code>fdist['三体']</code> 词频</li>
<li><code>fdist.plot(20,cumulative=True)</code> 前20个词画图</li>
</ol>
</li>
<li><p>ConditionFreqDist 带条件的频率分布类</p>

<ol>
<li></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>


<p>```python
cfd = ConditionalFreqDist(</p>

<pre><code>     [(conditon1, event1),(conditon2, event1),(conditon1, event2)...(conditon_n, event_n)]
   )
</code></pre>

<p>```</p>

<pre><code>     2. 
</code></pre>

<p>```python
cfd = ConditionalFreqDist(</p>

<pre><code>     (file, word)
     for file in files     
     for word in text_split(file))

#计算前10大词在各类文本中的频数
cfd.tabulate(conditions=files, samples=words)
</code></pre>

<p>```</p>

<h3>可视化</h3>

<ol>
<li><p>pyecharts &ndash; 动态交互可视化库</p>

<ol>
<li>Bar, Bar3D, Pie, Map, &hellip;</li>
</ol>
</li>
</ol>


<p>```python
&ldquo;&rdquo;&ldquo;
kwargs为通用配置项
&rdquo;&ldquo;&rdquo;
add(name, x_axis, y_axis, data,
   grid3d_opacity=1,
   grid3d_shading=&lsquo;color&rsquo;, **kwargs)</p>

<p>from pyecharts import Bar3D</p>

<p>bar3d = Bar3D(&ldquo;3D 柱状图示例&rdquo;, width=1200, height=600</p>

<pre><code>         ...
         ...
</code></pre>

<p>bar3d.add(
   &ldquo;&rdquo;,
   x_axis,
   y_axis,
   [[d[1], d[0], d[2]] for d in data],
   is_visualmap=True,
   visual_range=[0, 20],
   visual_range_color=range_color,
   grid3d_width=200,
   grid3d_depth=80,
)
bar3d.render(&lsquo;html/3D_Bar.html&rsquo;)
```</p>

<ol>
<li>. WordCloud</li>
</ol>


<p>```python
&ldquo;&rdquo;&ldquo;
name    str图例名称
attr    list属性名称
value   list属性所对应的值
&rdquo;&ldquo;&rdquo;
add(name, attr, value,
   shape=&ldquo;circle&rdquo;,
   word_gap=20,
   word_size_range=None,
   rotate_step=45)</p>

<p>from pyecharts import WordCloud
words = [&lsquo;Python&rsquo;, &lsquo;文本分析&rsquo;, &lsquo;编程&rsquo;]
freqs = [100,200,50]
wordcloud = WordCloud(width=1300, height=620)
wordcloud.add(&ldquo;&rdquo;, words, freqs, word_size_range=[20, 100])
wordcloud.render(&lsquo;html/wordcloud.html&rsquo;)
```</p>

<h3>数据分析库</h3>

<ol>
<li><p>pandas</p>

<ol>
<li><p>Series</p>

<ol>
<li>Series(list), Series(dict)</li>
<li>notnull, s2>2, s2[s2>2]</li>
<li>数学运算</li>
<li>value_counts(normalize=False)</li>
<li>head, tail, sample</li>
<li>plot(kind=&lsquo;bar&rsquo;)</li>
</ol>
</li>
<li><p>DataFrame</p>

<ol>
<li>列向量和行向量组成的数据结构
<code>python
import numpy as np
import pandas as pd
df = pd.DataFrame(np.arange(40).reshape(8,5),
       index=['one','two','three','four','five','six','seven','eight'],
       columns=['a','b','c','d','e'])
</code></li>
<li><p><code>df.drop('one', axis='rows', inplace=False)</code></p></li>
<li><p><code>df['a']</code>, <code>df.a</code> &ndash; 索引列名称</p></li>
<li><p><code>df.index</code> &ndash; 行索引</p></li>
<li><p><code>df[:3]</code> &ndash; 行索引查询</p></li>
<li><p><code>df.loc[['two','three'], ['a','d']]</code> &ndash; 子DataFrame查询</p></li>
<li><p><code>df.loc[:, ['a','d']]</code>, <code>df[['a', 'd']]</code> &ndash; 所有行, a,d 列</p></li>
<li><p><code>df.loc[['two','three'], :]</code> &ndash; 所有列</p></li>
<li><p><code>df.describe()</code></p></li>
<li><p><code>df['colname'] = 10009</code> &ndash; 新建或更新列</p></li>
<li><p>列方向进行<strong>聚合运算</strong></p></li>
</ol>
</li>
</ol>
</li>
</ol>


<p>```python
df.agg({&lsquo;colname1&rsquo;: [&lsquo;mean&rsquo;, diyfunc],</p>

<pre><code>   'colname2': ['mean', 'sum']})
</code></pre>

<p>```</p>

<pre><code>    *. `df.agg('mean')`- 针对所有列的聚合操作
  13. 针对值进行分组 - **groupby**
      1. `df.set_index('date').groupby('name')['ext price'].resample("M").sum()` - 针对name列的值进行分组
      2. `df.groupby(['name', pd.Grouper(key='date', freq='M')])['ext price'].sum()`
      3. `df.groupby('violation_raw').search_conducted.agg(['mean', 'count'])`
  14. `newdf=df+100`
  15. `newdf&gt;120`
  16. `df[newdf&gt;120]`
  17. **时间操作**
      1. pd.date_range() 生成一个时间段索引
         1. `pd.date_range('2017/10/11 10:00:01', '20181030', freq='3M') `
      2. pd.to_datetime(data) 将data序列数据转换为datetime类型Series
</code></pre>

<p><code>python
datatime = pd.to_datetime(data)
datatime.dt.year
datatime.dt.month.value_counts()
</code></p>

<pre><code>      3. 指定日期为索引
         1. 
</code></pre>

<p><code>python
tm_rng = pd.date_range('2017-12-31 12:00:00',periods=20, freq='5H')
tm_series = pd.Series(np.random.randn(len(tm_rng)), index=tm_rng)
tm_series['2017']
</code></p>

<pre><code>      4. resample
         1. `df.set_index('date').resample('2M')['ext price'].sum()`
  18. 读取csv或excell文件为dataFrame
      1. `df = pd.read_excel('data/sample-salesv3.xlsx', encoding='gbk')`
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[浅谈软件开发]]></title>
    <link href="http://www.fengjimin.com/blog/2013/09/02/an-engineers-view-of-application-development/"/>
    <updated>2013-09-02T20:47:00+08:00</updated>
    <id>http://www.fengjimin.com/blog/2013/09/02/an-engineers-view-of-application-development</id>
    <content type="html"><![CDATA[<blockquote><p>第一篇日志写于2013年。2015年的今日，经历一个充实的互联网业务的开发周期，回头看看，理解又有不同。说到底软件开发是定义问题，解决问题，二者合一。开发的过程绝不仅仅是写代码，测试，上线，支持。而是始终围绕其中的一个主题：用户、业务、产品、开发，各种角色不断沟通，提出各自理解，以各种形式如描叙、图表、代码、产品、UI、体验。。。这一切的一切，都是软件开发的一部分，每一个部分，都有同样的重要性，都可以遵循同样的方法论。最终的目标，是所有stakeholder达到一致的理解，从而从中得到自己想要的价值。</p></blockquote>

<!--more-->


<blockquote><p>另一方面，目前主流软件开发的模式，是面向对象编程，是基于OOP之上的设计模式。这是基础思想方法，是开发人员赖以搭高楼、赖以组装赛车的百宝箱。面向对象主要的思想来源，在于概念的不同抽象层次，对应了父类子类关系，既符合重用原则，也更符合人类思维。
当然，现实由于其复杂性，各种问题各种关连。从现实的问题中找到最基本最具重要性的维度来抽象，这需要一个匠人的直觉。这也是软件开发最有趣最激动人心的部分。没有武功秘籍，没有万能钥匙。真正有的，只存乎一心。。。</p></blockquote>

<p>从大学学编程开始，到如今学习工作的经验也有十多年了。记得大学毕业一、二年的时候和两个朋友讨论到同批不少同学转行去做金融，表达了羡慕之情，但那两个朋友异口同声的说其实我还蛮适合做IT的。姑且当作他们发现我身上适合搞IT做软件的特质吧。既然在这一行坚持了下来，也有必要做一个总结回顾，目的是从经验教训中找到规律，得到一个更清晰的视野。</p>

<p>软件开发，如同建房子、文艺创作，是创造的活动。任何创造的活动，需要工具，也需要想象力，完成一个复杂的项目则需要多人协作。创造，也是左脑和右脑协作的结果。左脑负责计算，逻辑推理，右脑负责情景想象，左右脑合作，才能做出符合需求、同时保证质量的产品。</p>

<p>关乎右脑想象力的方面，在建筑行业，想象力比较容易得到实物参照，一栋三维的建筑模型可以把成品外形模仿得八九不离十。而创作文学、绘画、音乐，是对现实的描叙或理念的抽象，似乎更关乎右脑记忆和情感的功能。而软件的设计，则是要满足用户的信息和功能的需求，这种需求难以有固化的实物参照，是一种概念的抽象和概念关系的梳理，且需要精确固化；它也不像文学和音乐可以有生动的现实做参照、表达的理念不需要精确。</p>

<p>另一方面关乎左脑的逻辑推理。一个建筑工程需要对工程涉及的各要素如人力、材料等进行必要测算，技术方面主要是结构力学和工程管理的应用。文艺的创作是对文字或绘画、音乐元素的组合，需要的是有对运用这些元素进行自我表达的感觉(Feel)。而软件的设计开发，需要把握编程语言，功能框架（函数库），运行机理，以及合理运用他们来表达清晰的概念关系，其最终产品不仅需要面向终端用户，也需要面向技术人员（包括自己），保证可维护和可扩展。</p>

<p>需求实现是应用开发的缘起。可以根据操作信息的不同类型、面向用户提供的功能进行分类，比如多媒体应用，网络应用，游戏应用，大数据应用，等等。在我多年应用开发的经验中，需求的理解是重要一环。其难点是需求与所运用技术的要求或限制的清晰区分。往往在思考需求的时候，缺乏经验的程序员会与所要运用的技术实现的要求和限制混为一谈。比如 Java deploy 的环境要求。只需要记住，所有的需求都是面向软件用户的。这种需求是完全独立于实现的语言或者框架的，可以用更高级的语言（甚至自然语言）来进行表达的。</p>

<p>大多数应用可以分为前端和后端来开发，也就可以运用MVC模式（视图-模型-控制器）。为保证维护性和可扩展性，需要使用DRY和正交设计的等等原则。本文和以后的系列博文，我将根据自己应用开发的经验，对不同需求的应用开发过程进行总结，尽量找到内在规律，推广到普遍软件开发的工作中，提高效率和质量。</p>
]]></content>
  </entry>
  
</feed>
